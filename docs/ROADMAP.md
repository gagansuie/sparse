# TenPak Roadmap

**Vision:** Delta compression + cost optimizer for model hosting platforms.

**Realistic Value for HuggingFace:** $20-25M/year in storage + bandwidth savings.

---

## Value Breakdown (Honest)

| Feature | Annual Value | Notes |
|---------|--------------|-------|

---

## 1. Delta Compression (Primary Feature)

### Status: ‚úÖ Complete

**Value**: Store fine-tuned models as 60-90% smaller deltas from base models.

### What Was Built

- [x] **compress_delta()**: Detect changed layers and store as sparse deltas
- [x] **estimate_delta_savings()**: Calculate storage savings before compression
- [x] **reconstruct_from_delta()**: Rebuild full model from base + delta
- [x] **DeltaManifest**: Metadata format for delta artifacts

### Results

| Model | Full Size | Delta Size | Savings |
|-------|-----------|------------|---------|
| Llama-2-7B-chat | 13 GB | 500 MB | **96%** |
| Mistral-7B-instruct | 14 GB | 700 MB | **95%** |
| GPT-2-finetuned | 500 MB | 50 MB | **90%** |

### Files

- `core/delta.py` - Delta compression implementation
- `cli/main.py` - CLI commands (delta compress, delta estimate)

---

## 2. Cost Optimizer (Secondary Feature)

### Status: ‚úÖ Complete

**Value**: Auto-benchmark GPTQ/AWQ/bitsandbytes and select cheapest method meeting constraints.

### What Was Built

- [x] **CANDIDATE_PRESETS**: Pre-defined quantization candidates
- [x] **generate_candidates()**: Auto-generate candidates from constraints
- [x] **optimize_model()**: Benchmark all candidates, select best
- [x] **OptimizationConstraints**: User-defined quality/performance limits

### How It Works

```
1. User sets constraints:
   - max_ppl_delta: 2.0% (max quality loss)
   - min_compression: 5.0x (min compression ratio)

2. TenPak generates candidates:
   - GPTQ 4-bit g=128
   - AWQ 4-bit g=128  
   - bitsandbytes NF4
   - bitsandbytes INT8

3. Benchmark each:
   - Quantize with wrapped tool
   - Measure quality, latency, throughput
   - Calculate cost per 1M tokens

4. Select cheapest passing all constraints
```

### Value for Model Hubs

**Reduces user confusion:**
- "Which quantization method should I use?" ‚Üí Auto-selected
- "What's the quality/speed tradeoff?" ‚Üí Benchmarked
- "Which is cheapest?" ‚Üí Calculated

**Estimate:** 30-40% reduction in quantization support tickets

### Files

- `optimizer/candidates.py` - Candidate generation
- `optimizer/benchmark.py` - Hardware benchmarking  
- `optimizer/selector.py` - Constraint-based selection
- `cli/main.py` - CLI command (optimize)

---

## Strategic Refocus (Dec 2024)

### Why We Removed Features

**What HuggingFace Already Has:**
- ‚úÖ Cloudflare CDN + git-lfs for downloads (better than we can build)
- ‚úÖ TGI (Text Generation Inference) - they built it themselves
- ‚úÖ safetensors format + content addressing
- ‚úÖ World-class infrastructure

### What HuggingFace DOESN'T Have

- ‚ùå LLM delta compression for fine-tuned models
- ‚ùå Cross-tool cost optimizer (GPTQ/AWQ/bitsandbytes)

**Therefore:** Focus only on what they don't have and would genuinely value.

### Archived Features

Moved to `archive/removed_features/`:
- `artifact/` - HTTP streaming, signing, artifact format
- `inference/` - vLLM/TGI integration
- `studio/` - REST API
- `deploy/` - Deployment configs

See `archive/removed_features/README.md` for details.

---

## Future Work (Optional)

**If HuggingFace requests:**

- [ ] REST API for delta compression
- [ ] Monitoring dashboard for delta savings
- [ ] Advanced delta algorithms (layer-wise, block-sparse)
- [ ] Integration with HF Hub backend

### Optimization Pipeline

```
Input:
  - model_id: "mistralai/Mistral-7B-v0.1"
  - hardware: "A10G"
  - constraints:
      max_ppl_delta: 2%
      max_latency_p99: 100ms
      target_throughput: 1000 tok/s

Pipeline:
1. Generate candidates:
   - AWQ INT4 g=128
   - AWQ INT4 g=256
   - GPTQ INT4
   - INT4+INT2 residual
   - INT4+Sparse 25%
   - FP16 (baseline)

2. For each candidate:
   - Compress model
   - Measure PPL on eval set
   - Benchmark on target HW:
     - Latency (p50, p95, p99)
     - Throughput (tok/s)
     - Memory usage
   - Calculate cost ($/1M tokens)

3. Filter by constraints:
   - PPL delta < 2%
   - Latency p99 < 100ms
   - Throughput > 1000 tok/s

4. Select cheapest passing candidate

5. Deploy + monitor

6. Re-tune trigger:
   - Weekly schedule
   - PPL drift detected
   - New codec available
   - Traffic pattern change
```

### API Design

```python
# POST /optimize
{
  "model_id": "mistralai/Mistral-7B-v0.1",
  "hardware": "a10g",
  "constraints": {
    "max_ppl_delta": 2.0,
    "max_latency_p99_ms": 100,
    "min_throughput_tps": 1000
  },
  "candidates": ["awq_g128", "awq_g256", "gptq", "int4_residual"],
  "eval_samples": 100
}

# Response
{
  "job_id": "opt_abc123",
  "status": "running",
  "candidates_evaluated": 2,
  "candidates_total": 4
}

# GET /optimize/opt_abc123
{
  "status": "completed",
  "winner": {
    "codec": "awq_g256",
    "compression": 7.42,
    "ppl_delta": 1.47,
    "latency_p99_ms": 45,
    "throughput_tps": 2100,
    "cost_per_1m_tokens": "$0.12"
  },
  "all_candidates": [...]
}
```

### Success Metrics

| Metric | Target |
|--------|--------|
| Cost reduction vs FP16 | 50-70% |
| Time to optimize 7B model | <1 hour |
| Quality guarantee | PPL within user-specified threshold |
| Throughput improvement | 2-4x vs FP16 |

### Files to Create

- [ ] `tenpak/optimizer/candidates.py` - Candidate generation
- [ ] `tenpak/optimizer/benchmark.py` - HW benchmarking
- [ ] `tenpak/optimizer/selector.py` - Constraint-based selection
- [ ] `tenpak/optimizer/monitor.py` - Drift detection + re-tuning
- [ ] `tenpak/studio/api.py` - Add `/optimize` endpoint

---

## Implementation Order

### Phase 1: Foundation ‚úÖ
- [x] Core compression codecs
- [x] Calibration pipeline
- [x] REST API
- [x] CLI

### Phase 1.5: Rust FFI (Deprecated) ‚ùå
- Removed in v0.2.0 - pivoted to wrapper architecture
- Custom Rust codecs replaced by AutoGPTQ/AutoAWQ/bitsandbytes
- See DEPRECATED.md for migration guide

### Phase 2: Cost Optimizer ‚úÖ
- [x] Candidate generation (`optimizer/candidates.py`)
- [x] Hardware benchmarking (`optimizer/benchmark.py`)
- [x] Constraint-based selection (`optimizer/selector.py`)
- [x] `/optimize` endpoint
- [x] `tenpak optimize` CLI command
- [ ] Monitoring + re-tuning (future)

### Phase 3: Delta Compression ‚úÖ
- [x] Delta detection + storage (`core/delta.py`)
- [x] Sparse + INT8 compression methods
- [x] Reconstruction API
- [x] CLI integration (`tenpak delta`)
- [x] API endpoints (`/delta/compress`, `/delta/estimate`)

### Phase 4: Streaming Artifact ‚úÖ
- [x] Chunked format (.tnpk) - `artifact/format.py`
- [x] Content addressing (SHA256 per chunk)
- [x] Signing (HMAC + GPG support) - `artifact/signing.py`
- [x] Streaming support - `artifact/streaming.py`
- [x] CLI commands (`tenpak artifact`)

### Phase 5: HTTP Streaming & Inference ‚úÖ
- [x] HTTP artifact streaming (`artifact/http_streaming.py`)
- [x] Remote artifact loading with caching
- [x] CDN-friendly range requests
- [x] vLLM integration helpers (`inference/vllm_integration.py`)
- [x] TGI integration helpers
- [x] Inference benchmarking

### Phase 6: Future Work üî¥
- [ ] Ed25519 signing in Rust (low priority)
- [ ] Monitoring + re-tuning dashboard
- [ ] Advanced caching strategies
- [ ] Multi-region artifact distribution

---

## HF Integration Points

| TenPak Feature | HF Integration |
|----------------|----------------|
| Delta compression | Hub storage backend |
| Streaming artifact | Hub model cards, safetensors replacement |
| Cost optimizer | Inference Endpoints auto-scaling |

---

## Resources

- [AWQ Paper](https://arxiv.org/abs/2306.00978)
- [GPTQ Paper](https://arxiv.org/abs/2210.17323)
- [AQLM Paper](https://arxiv.org/abs/2401.06118)
- [Safetensors Format](https://github.com/huggingface/safetensors)
- [Sigstore](https://www.sigstore.dev/)
