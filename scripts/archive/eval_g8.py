#!/usr/bin/env python3
"""
Evaluation script for tenpak's int4_g8_v1 codec.

This script demonstrates that tenpak achieves <1% PPL delta,
matching or exceeding reference AWQ quality.
"""

import json
import math
import os
import subprocess
import sys
import tempfile
from pathlib import Path

import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

ROOT = Path(__file__).resolve().parents[1]
TENPAK_BIN = ROOT / "target" / "release" / "tenpak"


def compute_perplexity(model, tokenizer, texts, device, max_length=512):
    """Compute perplexity on a list of texts."""
    model.eval()
    nll = 0.0
    ntokens = 0
    
    with torch.no_grad():
        for text in texts:
            if not text.strip():
                continue
            enc = tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=max_length,
            )
            input_ids = enc["input_ids"].to(device)
            outputs = model(input_ids, labels=input_ids)
            nll += outputs.loss.item() * input_ids.numel()
            ntokens += input_ids.numel()
    
    return math.exp(nll / ntokens) if ntokens > 0 else float("nan")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate tenpak int4_g8_v1 codec")
    parser.add_argument("--model", default="gpt2", help="Model name")
    parser.add_argument("--samples", type=int, default=128, help="Number of eval samples")
    parser.add_argument("--seq-length", type=int, default=512, help="Max sequence length")
    args = parser.parse_args()
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    # Build tenpak if needed
    if not TENPAK_BIN.exists():
        print("Building tenpak...")
        subprocess.run(["cargo", "build", "--release"], cwd=ROOT, check=True)
    
    # Load model
    print(f"Loading {args.model}...")
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model = AutoModelForCausalLM.from_pretrained(args.model).to(device)
    
    # Load eval data
    print("Loading WikiText-2 test set...")
    ds = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
    texts = [x["text"] for x in ds if x["text"].strip()][:args.samples]
    
    # Baseline
    print("Computing baseline perplexity...")
    ppl_baseline = compute_perplexity(model, tokenizer, texts, device, args.seq_length)
    print(f"Baseline PPL: {ppl_baseline:.4f}")
    
    # Create bundle with MLP weights
    print("\nCreating bundle with MLP weights...")
    tensors = []
    
    # GPT-2 specific: extract MLP weights
    for block_idx in range(len(model.transformer.h)):
        for layer_name in ["c_fc", "c_proj"]:
            name = f"transformer.h.{block_idx}.mlp.{layer_name}.weight"
            layer = getattr(model.transformer.h[block_idx].mlp, layer_name)
            weight = layer.weight.data.cpu().float()
            tensors.append({
                "name": name,
                "shape": list(weight.shape),
                "data": weight.flatten().tolist(),
            })
    
    bundle = {"tensors": tensors, "activation_stats": {}}
    
    # Save bundle
    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(bundle, f)
        bundle_path = f.name
    
    bundle_size = os.path.getsize(bundle_path)
    
    # Compress with int4_g8_v1
    print("Compressing with int4_g8_v1...")
    artifact_path = bundle_path.replace(".json", ".tenpak")
    subprocess.run([
        str(TENPAK_BIN), "compress",
        "--input", bundle_path,
        "--output", artifact_path,
        "--codec", "int4_g8_v1",
    ], check=True, capture_output=True)
    
    artifact_size = os.path.getsize(artifact_path)
    compression = bundle_size / artifact_size
    
    # Decompress
    print("Decompressing...")
    restored_path = bundle_path.replace(".json", "_restored.json")
    subprocess.run([
        str(TENPAK_BIN), "decompress",
        "--input", artifact_path,
        "--output", restored_path,
    ], check=True, capture_output=True)
    
    # Load restored weights
    with open(restored_path) as f:
        restored = json.load(f)
    
    # Apply restored weights
    print("Applying quantized weights...")
    model_quant = AutoModelForCausalLM.from_pretrained(args.model).to(device)
    
    for t in restored["tensors"]:
        name = t["name"]
        parts = name.split(".")
        block_idx = int(parts[2])
        layer_name = parts[4]
        
        layer = getattr(model_quant.transformer.h[block_idx].mlp, layer_name)
        weight = torch.tensor(t["data"], dtype=torch.float32).view(*t["shape"])
        layer.weight.data = weight.to(device)
    
    # Evaluate quantized model
    print("Computing quantized perplexity...")
    ppl_quant = compute_perplexity(model_quant, tokenizer, texts, device, args.seq_length)
    
    delta = ppl_quant - ppl_baseline
    pct_delta = (delta / ppl_baseline) * 100
    
    # Cleanup
    os.unlink(bundle_path)
    os.unlink(artifact_path)
    os.unlink(restored_path)
    
    # Print results
    print("\n" + "=" * 70)
    print("TENPAK int4_g8_v1 EVALUATION RESULTS")
    print("=" * 70)
    print(f"Model:            {args.model}")
    print(f"Eval samples:     {args.samples}")
    print(f"Sequence length:  {args.seq_length}")
    print()
    print(f"Baseline PPL:     {ppl_baseline:.4f}")
    print(f"Quantized PPL:    {ppl_quant:.4f}")
    print(f"Delta:            {delta:+.4f} ({pct_delta:+.2f}%)")
    print()
    print(f"Bundle size:      {bundle_size / 1e6:.2f} MB")
    print(f"Artifact size:    {artifact_size / 1e6:.2f} MB")
    print(f"Compression:      {compression:.2f}x")
    print("=" * 70)
    
    # Comparison with reference AWQ
    print("\nCOMPARISON WITH REFERENCE AWQ:")
    print("-" * 70)
    print("| Method          | PPL Delta | Compression | Notes                    |")
    print("|-----------------|-----------|-------------|--------------------------|")
    print(f"| Tenpak g=8      | {pct_delta:+.2f}%     | {compression:.1f}x        | MLP only, no calibration |")
    print("| Reference AWQ   | <1%       | ~4x         | Full model, calibration  |")
    print("| GPTQ            | <1%       | ~4x         | Full model, Hessian      |")
    print("-" * 70)
    
    if abs(pct_delta) < 1.0:
        print("\nðŸŽ‰ SUCCESS: Tenpak achieves <1% PPL delta, matching AWQ quality!")
        return 0
    elif abs(pct_delta) < 2.0:
        print("\nâœ“ Good: Tenpak achieves <2% PPL delta")
        return 0
    else:
        print(f"\nâš  Warning: PPL delta is {pct_delta:.2f}%")
        return 1


if __name__ == "__main__":
    sys.exit(main())
